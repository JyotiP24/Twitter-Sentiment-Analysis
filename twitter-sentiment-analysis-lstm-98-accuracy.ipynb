{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2186313,"sourceType":"datasetVersion","datasetId":1312443},{"sourceId":9049593,"sourceType":"datasetVersion","datasetId":5448891}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <a id=\"Import\"></a>\n# <p style=\"background-color: #422057FF; font-family: 'Copperplate'; color:#FDDB27FF; font-size:140%; text-align:center; border-radius:1000px 10px;\">1.0 About Author</p> \n\n<div style=\"border: 2px solid #006B38FF; padding: 10px; max-width: 1500px;\">\n    <p>\n        I am <b>Atif Ali Khokhar</b>, a passionate data scientist dedicated to mastering machine learning techniques and continually expanding my knowledge base. I believe in the mantra of #KeepLearning and #KeepSupporting, as I am committed to constant growth and uplifting others in the field.\n    </p>\n    <div style=\"text-align: center;\">\n        <img src=\"https://media.licdn.com/dms/image/D4E03AQHbj9PaMNpUuQ/profile-displayphoto-shrink_400_400/0/1694879278829?e=1721260800&v=beta&t=XWss7C6pbhbBWJoETbMhxsQASHKpP9Vkf7qty24U6Hs\" alt=\"Profile Picture\" style=\"width: 100px; height: 100px; border-radius: 50%; border: 2px solid #D35400;\"><br>\n    </div>\n    <p>\n        You can find more about me on my <a href=\"https://www.linkedin.com/in/atifalikhokhar/\" target=\"_blank\">LinkedIn</a>.<br>\n        Feel free to connect and reach out for any collaboration or queries!\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"Import\"></a>\n# <p style=\"background-color: #422057FF; font-family: 'Copperplate'; color:#FDDB27FF; font-size:140%; text-align:center; border-radius:1000px 10px;\">2.0 About Project and Data</p> \n\n## Project Title: Sentiment Sleuth: Cracking Twitter's Emotional Code with LSTM and NLP\n\n### **Description:**\n   This Kaggle notebook, titled \"Twitter Sentiment Analysis,\" focuses on understanding and classifying the sentiment of tweets. Leveraging the power of Natural Language Processing (NLP) and Long Short-Term Memory (LSTM) networks, this project aims to accurately predict whether a given tweet expresses a positive or negative sentiment. By analyzing a dataset of tweets, the notebook demonstrates the entire workflow of data preprocessing, model training, evaluation, and visualization of results.\n    \n### **Objective:**\n   The primary objective of this notebook is to develop an efficient binary classification model that can categorize tweets into positive or negative sentiments. The specific goals include:\n\n**Data Preprocessing:** Clean and preprocess the raw tweet data, including tokenization, stop word removal, and text normalization.\n\n**Feature Engineering:** Transform the text data into numerical representations suitable for input into the LSTM model.\n\n**Model Building:** Construct and train an LSTM model optimized for text data to perform the binary classification task.\n\n**Evaluation:** Assess the model's performance using appropriate metrics such as accuracy, precision, recall, and F1-score.\n\n**Visualization:** Provide clear and insightful visualizations of the results to interpret the model's effectiveness and areas for improvement.\n\n### **Dataset:**\n[sentimental-analysis-for-tweets](/kaggle/input/sentimental-analysis-for-tweets)","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"Import\"></a>\n# <p style=\"background-color: #422057FF; font-family: 'Copperplate'; color:#FDDB27FF; font-size:140%; text-align:center; border-radius:1000px 10px;\">3.0 Importing Libraries and Checking Data</p> ","metadata":{}},{"cell_type":"markdown","source":"# **3.1 Importing Libraries** ","metadata":{}},{"cell_type":"code","source":"# Import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom bs4 import BeautifulSoup\nimport string\nimport re\nimport emoji\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom keras.layers import LSTM, Dense, SimpleRNN, Embedding, Flatten, Dropout\nfrom keras.activations import softmax","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:50:13.591688Z","iopub.execute_input":"2024-07-26T12:50:13.592053Z","iopub.status.idle":"2024-07-26T12:50:13.600076Z","shell.execute_reply.started":"2024-07-26T12:50:13.592026Z","shell.execute_reply":"2024-07-26T12:50:13.599035Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3.2 Loading Dataset**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/sentimental-analysis-for-tweets/sentiment_tweets3.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.064077Z","iopub.execute_input":"2024-07-26T12:45:36.065028Z","iopub.status.idle":"2024-07-26T12:45:36.170678Z","shell.execute_reply.started":"2024-07-26T12:45:36.064991Z","shell.execute_reply":"2024-07-26T12:45:36.169892Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.171774Z","iopub.execute_input":"2024-07-26T12:45:36.172051Z","iopub.status.idle":"2024-07-26T12:45:36.195084Z","shell.execute_reply.started":"2024-07-26T12:45:36.172026Z","shell.execute_reply":"2024-07-26T12:45:36.194195Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.197066Z","iopub.execute_input":"2024-07-26T12:45:36.19736Z","iopub.status.idle":"2024-07-26T12:45:36.202878Z","shell.execute_reply.started":"2024-07-26T12:45:36.197334Z","shell.execute_reply":"2024-07-26T12:45:36.201963Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.203915Z","iopub.execute_input":"2024-07-26T12:45:36.204164Z","iopub.status.idle":"2024-07-26T12:45:36.220228Z","shell.execute_reply.started":"2024-07-26T12:45:36.204141Z","shell.execute_reply":"2024-07-26T12:45:36.219019Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.columns = ['Index','Text', 'label']\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.221877Z","iopub.execute_input":"2024-07-26T12:45:36.222213Z","iopub.status.idle":"2024-07-26T12:45:36.258159Z","shell.execute_reply.started":"2024-07-26T12:45:36.222188Z","shell.execute_reply":"2024-07-26T12:45:36.256949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"Import\"></a>\n# <p style=\"background-color: #422057FF; font-family: 'Copperplate'; color:#FDDB27FF; font-size:140%; text-align:center; border-radius:1000px 10px;\">4.0 Data Preprocessing Steps</p> ","metadata":{}},{"cell_type":"markdown","source":"# **4.1 Lowercasing the Text :**","metadata":{}},{"cell_type":"code","source":"data['Text'] = data['Text'].str.lower()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.259719Z","iopub.execute_input":"2024-07-26T12:45:36.260143Z","iopub.status.idle":"2024-07-26T12:45:36.311793Z","shell.execute_reply.started":"2024-07-26T12:45:36.260098Z","shell.execute_reply":"2024-07-26T12:45:36.310871Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4.2 Remove HTML tags :**","metadata":{}},{"cell_type":"code","source":"def remove_html_tags(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\ndata['Text'] = data['Text'].apply(remove_html_tags)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.312934Z","iopub.execute_input":"2024-07-26T12:45:36.313295Z","iopub.status.idle":"2024-07-26T12:45:36.842989Z","shell.execute_reply.started":"2024-07-26T12:45:36.313261Z","shell.execute_reply":"2024-07-26T12:45:36.842085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4.3 Remove URLs :**","metadata":{}},{"cell_type":"code","source":"def remove_urls(text):\n    return re.sub(r'http\\S+|www\\S+', '', text)\n\ndata['Text'] = data['Text'].apply(remove_urls)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.846943Z","iopub.execute_input":"2024-07-26T12:45:36.847281Z","iopub.status.idle":"2024-07-26T12:45:36.894755Z","shell.execute_reply.started":"2024-07-26T12:45:36.847255Z","shell.execute_reply":"2024-07-26T12:45:36.893694Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4.4 Remove punctuation :**","metadata":{}},{"cell_type":"code","source":"punctuation = string.punctuation\n\n# Function to remove punctuation from text\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', punctuation))\n\n# Apply remove_punctuation function to 'Text' column\ndata['Text'] = data['Text'].apply(remove_punctuation)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.896296Z","iopub.execute_input":"2024-07-26T12:45:36.896945Z","iopub.status.idle":"2024-07-26T12:45:36.956514Z","shell.execute_reply.started":"2024-07-26T12:45:36.896918Z","shell.execute_reply":"2024-07-26T12:45:36.955669Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4.5 Handling ChatWords :**","metadata":{}},{"cell_type":"code","source":"chat_words = {\n    \"AFAIK\": \"As Far As I Know\",\n    \"AFK\": \"Away From Keyboard\",\n    \"ASAP\": \"As Soon As Possible\",\n    \"ATK\": \"At The Keyboard\",\n    \"ATM\": \"At The Moment\",\n    \"A3\": \"Anytime, Anywhere, Anyplace\",\n    \"BAK\": \"Back At Keyboard\",\n    \"BBL\": \"Be Back Later\",\n    \"BBS\": \"Be Back Soon\",\n    \"BFN\": \"Bye For Now\",\n    \"B4N\": \"Bye For Now\",\n    \"BRB\": \"Be Right Back\",\n    \"BRT\": \"Be Right There\",\n    \"BTW\": \"By The Way\",\n    \"B4\": \"Before\",\n    \"B4N\": \"Bye For Now\",\n    \"CU\": \"See You\",\n    \"CUL8R\": \"See You Later\",\n    \"CYA\": \"See You\",\n    \"FAQ\": \"Frequently Asked Questions\",\n    \"FC\": \"Fingers Crossed\",\n    \"FWIW\": \"For What It's Worth\",\n    \"FYI\": \"For Your Information\",\n    \"GAL\": \"Get A Life\",\n    \"GG\": \"Good Game\",\n    \"GN\": \"Good Night\",\n    \"GMTA\": \"Great Minds Think Alike\",\n    \"GR8\": \"Great!\",\n    \"G9\": \"Genius\",\n    \"IC\": \"I See\",\n    \"ICQ\": \"I Seek you (also a chat program)\",\n    \"ILU\": \"ILU: I Love You\",\n    \"IMHO\": \"In My Honest/Humble Opinion\",\n    \"IMO\": \"In My Opinion\",\n    \"IOW\": \"In Other Words\",\n    \"IRL\": \"In Real Life\",\n    \"KISS\": \"Keep It Simple, Stupid\",\n    \"LDR\": \"Long Distance Relationship\",\n    \"LMAO\": \"Laugh My A.. Off\",\n    \"LOL\": \"Laughing Out Loud\",\n    \"LTNS\": \"Long Time No See\",\n    \"L8R\": \"Later\",\n    \"MTE\": \"My Thoughts Exactly\",\n    \"M8\": \"Mate\",\n    \"NRN\": \"No Reply Necessary\",\n    \"OIC\": \"Oh I See\",\n    \"PITA\": \"Pain In The A..\",\n    \"PRT\": \"Party\",\n    \"PRW\": \"Parents Are Watching\",\n    \"QPSA?\": \"Que Pasa?\",\n    \"ROFL\": \"Rolling On The Floor Laughing\",\n    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n    \"SK8\": \"Skate\",\n    \"STATS\": \"Your sex and age\",\n    \"ASL\": \"Age, Sex, Location\",\n    \"THX\": \"Thank You\",\n    \"TTFN\": \"Ta-Ta For Now!\",\n    \"TTYL\": \"Talk To You Later\",\n    \"U\": \"You\",\n    \"U2\": \"You Too\",\n    \"U4E\": \"Yours For Ever\",\n    \"WB\": \"Welcome Back\",\n    \"WTF\": \"What The F...\",\n    \"WTG\": \"Way To Go!\",\n    \"WUF\": \"Where Are You From?\",\n    \"W8\": \"Wait...\",\n    \"7K\": \"Sick:-D Laugher\",\n    \"TFW\": \"That feeling when\",\n    \"MFW\": \"My face when\",\n    \"MRW\": \"My reaction when\",\n    \"IFYP\": \"I feel your pain\",\n    \"TNTL\": \"Trying not to laugh\",\n    \"JK\": \"Just kidding\",\n    \"IDC\": \"I don't care\",\n    \"ILY\": \"I love you\",\n    \"IMU\": \"I miss you\",\n    \"ADIH\": \"Another day in hell\",\n    \"ZZZ\": \"Sleeping, bored, tired\",\n    \"WYWH\": \"Wish you were here\",\n    \"TIME\": \"Tears in my eyes\",\n    \"BAE\": \"Before anyone else\",\n    \"FIMH\": \"Forever in my heart\",\n    \"BSAAW\": \"Big smile and a wink\",\n    \"BWL\": \"Bursting with laughter\",\n    \"BFF\": \"Best friends forever\",\n    \"CSL\": \"Can't stop laughing\"\n}\n\n# Function to replace chat words with their full forms\ndef replace_chat_words(text):\n    words = text.split()\n    for i, word in enumerate(words):\n        if word.lower() in chat_words:\n            words[i] = chat_words[word.lower()]\n    return ' '.join(words)\n\n# Apply replace_chat_words function to 'Text' column\ndata['Text'] = data['Text'].apply(replace_chat_words) \n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:36.957575Z","iopub.execute_input":"2024-07-26T12:45:36.957845Z","iopub.status.idle":"2024-07-26T12:45:37.03349Z","shell.execute_reply.started":"2024-07-26T12:45:36.957822Z","shell.execute_reply":"2024-07-26T12:45:37.032606Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4.6 Handling StopWords :**","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n\n# Example usage\ndata['Text'] = data['Text'].apply(remove_stopwords)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:37.034667Z","iopub.execute_input":"2024-07-26T12:45:37.034936Z","iopub.status.idle":"2024-07-26T12:45:38.491995Z","shell.execute_reply.started":"2024-07-26T12:45:37.034913Z","shell.execute_reply":"2024-07-26T12:45:38.491035Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4.7 Handling Emojis :**","metadata":{}},{"cell_type":"code","source":"def remove_emojis(text):\n    return emoji.replace_emoji(text, replace='')\n\n\ndata['Text'] = data['Text'].apply(remove_emojis)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:38.493134Z","iopub.execute_input":"2024-07-26T12:45:38.493429Z","iopub.status.idle":"2024-07-26T12:45:39.587528Z","shell.execute_reply.started":"2024-07-26T12:45:38.493404Z","shell.execute_reply":"2024-07-26T12:45:39.586552Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\nps = PorterStemmer()\n\ndef stem_words(text):\n    return \" \".join([ps.stem(word) for word in text.split()])\n\ndata['Text'] = data['Text'].apply(stem_words)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:39.588715Z","iopub.execute_input":"2024-07-26T12:45:39.589001Z","iopub.status.idle":"2024-07-26T12:45:42.51169Z","shell.execute_reply.started":"2024-07-26T12:45:39.588977Z","shell.execute_reply":"2024-07-26T12:45:42.51087Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4.8 Train Test Split :**","metadata":{}},{"cell_type":"code","source":"X = data['Text']\ny = data['label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:42.512931Z","iopub.execute_input":"2024-07-26T12:45:42.513288Z","iopub.status.idle":"2024-07-26T12:45:42.527797Z","shell.execute_reply.started":"2024-07-26T12:45:42.513251Z","shell.execute_reply":"2024-07-26T12:45:42.526999Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4.9 Tokenization and Padding Sequences :**","metadata":{}},{"cell_type":"code","source":"# 9. Tokenization and Padding Sequences\n\ntokenizer = Tokenizer(oov_token = 'nothing')\ntokenizer.fit_on_texts(X_train)\ntokenizer.fit_on_texts(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:42.529449Z","iopub.execute_input":"2024-07-26T12:45:42.52976Z","iopub.status.idle":"2024-07-26T12:45:42.773203Z","shell.execute_reply.started":"2024-07-26T12:45:42.529734Z","shell.execute_reply":"2024-07-26T12:45:42.772226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.document_count","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:42.774442Z","iopub.execute_input":"2024-07-26T12:45:42.774824Z","iopub.status.idle":"2024-07-26T12:45:42.780434Z","shell.execute_reply.started":"2024-07-26T12:45:42.774791Z","shell.execute_reply":"2024-07-26T12:45:42.779584Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_sequences = tokenizer.texts_to_sequences(X_train)\nX_test_sequences = tokenizer.texts_to_sequences(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:42.781388Z","iopub.execute_input":"2024-07-26T12:45:42.781707Z","iopub.status.idle":"2024-07-26T12:45:42.941847Z","shell.execute_reply.started":"2024-07-26T12:45:42.781684Z","shell.execute_reply":"2024-07-26T12:45:42.941063Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Max Len in X_train_sequences\nmaxlen = max(len(tokens) for tokens in X_train_sequences)\nprint(\"Maximum sequence length (maxlen):\", maxlen)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:42.942881Z","iopub.execute_input":"2024-07-26T12:45:42.943162Z","iopub.status.idle":"2024-07-26T12:45:42.948842Z","shell.execute_reply.started":"2024-07-26T12:45:42.943137Z","shell.execute_reply":"2024-07-26T12:45:42.947919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform padding on X_train and X_test sequences\nX_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen, padding='post')\nX_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:42.950046Z","iopub.execute_input":"2024-07-26T12:45:42.95035Z","iopub.status.idle":"2024-07-26T12:45:43.00495Z","shell.execute_reply.started":"2024-07-26T12:45:42.950325Z","shell.execute_reply":"2024-07-26T12:45:43.004261Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the padded sequences for X_train and X_test\nprint(\"X_train_padded:\")\nprint(X_train_padded)\nprint(\"\\nX_test_padded:\")\nprint(X_test_padded)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:43.005946Z","iopub.execute_input":"2024-07-26T12:45:43.006204Z","iopub.status.idle":"2024-07-26T12:45:43.011934Z","shell.execute_reply.started":"2024-07-26T12:45:43.00618Z","shell.execute_reply":"2024-07-26T12:45:43.011131Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Input Size\n# Embedding Input Size / Vocabulary Size \ninput_Size = np.max(X_train_padded) + 1\ninput_Size","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:43.013149Z","iopub.execute_input":"2024-07-26T12:45:43.013917Z","iopub.status.idle":"2024-07-26T12:45:43.027037Z","shell.execute_reply.started":"2024-07-26T12:45:43.01389Z","shell.execute_reply":"2024-07-26T12:45:43.026177Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"Import\"></a>\n# <p style=\"background-color: #422057FF; font-family: 'Copperplate'; color:#FDDB27FF; font-size:140%; text-align:center; border-radius:1000px 10px;\">5.0 Modeling</p> ","metadata":{}},{"cell_type":"markdown","source":"# **5.1 Model Building:**","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(LSTM(256, input_shape=(75,1), return_sequences=True))  \n\nmodel.add(LSTM(128)) \n\nmodel.add(Dense(64, activation='relu'))  \nmodel.add(Dropout(0.01))\n\nmodel.add(Dense(1, activation='sigmoid')) ","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:43.028169Z","iopub.execute_input":"2024-07-26T12:45:43.028442Z","iopub.status.idle":"2024-07-26T12:45:45.066364Z","shell.execute_reply.started":"2024-07-26T12:45:43.028418Z","shell.execute_reply":"2024-07-26T12:45:45.065598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **5.2 Model  Compilation:**","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:45.070398Z","iopub.execute_input":"2024-07-26T12:45:45.070722Z","iopub.status.idle":"2024-07-26T12:45:45.099184Z","shell.execute_reply.started":"2024-07-26T12:45:45.070695Z","shell.execute_reply":"2024-07-26T12:45:45.098336Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **5.3 Model Training:**","metadata":{}},{"cell_type":"code","source":"# Model Train \nhistory = model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_test_padded, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:45:45.100096Z","iopub.execute_input":"2024-07-26T12:45:45.100326Z","iopub.status.idle":"2024-07-26T12:46:17.18313Z","shell.execute_reply.started":"2024-07-26T12:45:45.100305Z","shell.execute_reply":"2024-07-26T12:46:17.182162Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"Import\"></a>\n# <p style=\"background-color: #422057FF; font-family: 'Copperplate'; color:#FDDB27FF; font-size:140%; text-align:center; border-radius:1000px 10px;\">6.0 Prediction</p> ","metadata":{}},{"cell_type":"markdown","source":"# **6.1 Plotting the graph of Accuracy and Validation Accuracy:**","metadata":{}},{"cell_type":"code","source":"plt.title('Training Accuracy vs Validation Accuracy')\n\nplt.plot(history.history['accuracy'], color='red',label='Train')\nplt.plot(history.history['val_accuracy'], color='blue',label='Validation')\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:50:25.883935Z","iopub.execute_input":"2024-07-26T12:50:25.884669Z","iopub.status.idle":"2024-07-26T12:50:26.227071Z","shell.execute_reply.started":"2024-07-26T12:50:25.884634Z","shell.execute_reply":"2024-07-26T12:50:26.22611Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **6.2 Plotting the graph of Accuracy and Validation loss:**","metadata":{}},{"cell_type":"code","source":"plt.title('Training Loss vs Validation Loss')\n\nplt.plot(history.history['loss'], color='red',label='Train')\nplt.plot(history.history['val_loss'], color='blue',label='Validation')\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T12:50:33.513122Z","iopub.execute_input":"2024-07-26T12:50:33.513526Z","iopub.status.idle":"2024-07-26T12:50:33.844844Z","shell.execute_reply.started":"2024-07-26T12:50:33.513493Z","shell.execute_reply":"2024-07-26T12:50:33.843915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"Import\"></a>\n# <p style=\"background-color: #422057FF; font-family: 'Copperplate'; color:#FDDB27FF; font-size:140%; text-align:center; border-radius:1000px 10px;\">7.0 The End</p> ","metadata":{}}]}